% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mlutuning.R
\name{mlutuning}
\alias{mlutuning}
\title{MLU Hyperparameter Tuning}
\usage{
mlutuning(
  X,
  Y,
  f,
  ML = c("Lasso", "Ridge", "RF", "CIF", "XGB", "CB", "Torch", "Logit_lasso", "OLS",
    "NLLS_exp", "loglin", "OLSensemble"),
  Kcv = 5,
  rf.cf.ntree.grid = c(100, 300, 500),
  rf.depth.grid = c(2, 4, 6, Inf),
  mtry.grid = c(1, 3, 5),
  cf.depth.grid = c(2, 4, Inf),
  ensemblefolds.grid = c(5, 10),
  polynomial.Lasso.grid = c(1, 2, 3),
  polynomial.Ridge.grid = c(1, 2, 3),
  polynomial.Logit_lasso.grid = c(1, 2, 3),
  polynomial.OLS.grid = c(1, 2, 3),
  polynomial.NLLS_exp.grid = c(1, 2, 3),
  polynomial.loglin.grid = c(1, 2, 3),
  xgb.nrounds.grid = c(100, 200, 500),
  xgb.eta.grid = c(0.01, 0.1, 0.3),
  xgb.depth.grid = c(1, 3, 6),
  xgb.child_weight.grid = c(1, 5, 10),
  xgb.subsample.grid = c(0.5, 0.75, 1),
  xgb.colsample.grid = c(0.5, 0.75, 1),
  cb.iterations.grid = c(100, 500, 1000),
  cb.depth.grid = c(1, 3, 6, 10),
  cb.learning_rate.grid = c(0.01, 0.1, 0.3),
  cb.l2_leaf_reg.grid = c(1, 3, 5, 7, 9),
  torch.hidden_units.grid = list(c(64, 32), c(128, 64), c(256, 128, 64)),
  torch.learning_rate.grid = c(0.001, 0.01, 0.1),
  torch.dropout.grid = c(0, 0.2, 0.5),
  torch.epochs.grid = c(50, 100, 200),
  var_penalization = 0,
  OLSensemble = NULL,
  SL.library = NULL,
  subsample = NULL,
  verbose = TRUE
)
}
\arguments{
\item{X}{Dataframe containing all the features on which the model was estimated.}

\item{Y}{Vector containing the labels for which the model was estimated.}

\item{f}{Function of Yi and Yj defining the dependent variable.}

\item{ML}{Which machine learners to tune.}

\item{Kcv}{Number of folds for cross-validation.}

\item{rf.cf.ntree.grid}{How many trees should be grown when using RF or CIF.}

\item{rf.depth.grid}{How deep should trees be grown in RF. Inf means full depth (NULL in ranger).}

\item{mtry.grid}{How many variables to consider at each split in RF. Defaults floor(sqrt(ncol(X))) and floor(ncol(X)/3) are always tried.}

\item{ensemblefolds.grid}{Integer specifying how many folds to use in ensemble methods such as OLSensemble or SuperLearner.}

\item{polynomial.Lasso.grid}{Degree of polynomial to be fitted when using Lasso.}

\item{polynomial.Ridge.grid}{Degree of polynomial to be fitted when using Ridge.}

\item{polynomial.Logit_lasso.grid}{Degree of polynomial to be fitted when using Logit_lasso.}

\item{polynomial.OLS.grid}{Degree of polynomial to be fitted when using OLS.}

\item{polynomial.NLLS_exp.grid}{Degree of polynomial to be fitted when using NLLS_exp.}

\item{polynomial.loglin.grid}{Degree of polynomial to be fitted when using loglin.}

\item{xgb.nrounds.grid}{Number of boosting iterations for XGBoost.}

\item{xgb.eta.grid}{Learning rate for XGBoost.}

\item{xgb.depth.grid}{Maximum depth of trees for XGBoost.}

\item{xgb.child_weight.grid}{Minimum sum of instance weight needed in a child for XGBoost.}

\item{xgb.subsample.grid}{Subsample ratio of the training instances for XGBoost.}

\item{xgb.colsample.grid}{Subsample ratio of columns when constructing each tree for XGBoost.}

\item{cb.iterations.grid}{Number of boosting iterations for CatBoost.}

\item{cb.depth.grid}{Depth of the trees for CatBoost.}

\item{cb.learning_rate.grid}{Learning rate for CatBoost.}

\item{cb.l2_leaf_reg.grid}{L2 regularization coefficient for CatBoost.}

\item{torch.hidden_units.grid}{List of hidden layer configurations for neural networks.}

\item{torch.learning_rate.grid}{Learning rates for neural network training.}

\item{torch.dropout.grid}{Dropout rates for neural networks.}

\item{torch.epochs.grid}{Number of training epochs for neural networks.}

\item{var_penalization}{Penalization parameter for variance in RMSE calculation. Default is 0.}

\item{OLSensemble}{String vector specifying which learners should be used in OLS ensemble method.}

\item{SL.library}{String vector specifying which learners should be used in SuperLearner.}

\item{subsample}{Either NULL (use all data), a proportion between 0 and 1, or an integer number of observations to randomly subsample before creating pairs. This can significantly reduce computational time for tuning. Default is NULL.}

\item{verbose}{Logical specifying whether to print progress. Default is TRUE.}

\item{cf.depth}{How deep should trees be grown in CIF (Inf is full depth, default from partykit).}
}
\value{
Tuning results from ML::MLtuning().
}
\description{
Performs hyperparameter tuning for pairwise machine learning models across
multiple algorithms including Random Forest, XGBoost, CatBoost, and neural networks.
Uses symmetric features: levels (sum) and distances (absolute differences).
}
